{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatveiV/ML_Fin_Notebooks/blob/main/%D0%A2%D0%B5%D0%BC%D0%B0_14_BigData_%D0%B4%D0%BB%D1%8F_%D1%84%D0%B8%D0%BD%D0%B0%D0%BD%D1%81%D0%BE%D0%B2%D0%BE%D0%B3%D0%BE_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data vs Small Data в финансах"
      ],
      "metadata": {
        "id": "HBseC1ghYBmH"
      },
      "id": "HBseC1ghYBmH"
    },
    {
      "cell_type": "markdown",
      "id": "IQHI5cZ-hlXw",
      "metadata": {
        "id": "IQHI5cZ-hlXw"
      },
      "source": [
        "\n",
        "## Примеры применения больших данных в финансовой индустрии\n",
        "\n",
        "Большие данные открывают новые возможности для финансовой индустрии, позволяя компаниям улучшать свои продукты и услуги, а также принимать более обоснованные решения. Рассмотрим несколько примеров:\n",
        "\n",
        "### Управление рисками\n",
        "\n",
        "Финансовые учреждения используют большие данные для анализа рыночных тенденций и прогнозирования рисков. Это позволяет им разрабатывать стратегии управления рисками и минимизировать потенциальные убытки.\n",
        "\n",
        "### Персонализация услуг\n",
        "\n",
        "С помощью больших данных компании могут анализировать поведение клиентов и предлагать персонализированные продукты и услуги. Это повышает удовлетворенность клиентов и увеличивает их лояльность.\n",
        "\n",
        "### Обнаружение мошенничества\n",
        "\n",
        "Анализ больших данных позволяет выявлять аномалии и подозрительные активности, что помогает в обнаружении и предотвращении мошенничества. Это особенно важно для защиты финансовых транзакций и данных клиентов.\n",
        "\n",
        "### Инвестиционные стратегии\n",
        "\n",
        "Большие данные используются для разработки и оптимизации инвестиционных стратегий. Анализ исторических данных и текущих рыночных условий позволяет инвесторам принимать более обоснованные решения.\n",
        "\n",
        "### Прогнозирование рыночных трендов\n",
        "\n",
        "С помощью больших данных компании могут прогнозировать будущие рыночные тренды и адаптировать свои стратегии в соответствии с изменяющимися условиями. Это дает им конкурентное преимущество и позволяет оставаться на шаг впереди конкурентов.\n",
        "\n",
        "## Понимание различий между большими и малыми данными\n",
        "\n",
        "В современном мире данные играют ключевую роль в принятии решений, особенно в финансовой индустрии. Однако не все данные одинаковы, и понимание различий между большими и малыми данными является важным аспектом для эффективного использования информации.\n",
        "\n",
        "### Малые данные\n",
        "\n",
        "Малые данные представляют собой ограниченные наборы данных, которые могут быть легко обработаны и проанализированы с использованием традиционных инструментов и методов. Они часто структурированы и могут быть представлены в виде таблиц или баз данных. Примеры малых данных включают:\n",
        "- Финансовые отчеты компаний\n",
        "- Ежедневные транзакции\n",
        "- Исторические данные по акциям\n",
        "\n",
        "### Большие данные\n",
        "\n",
        "Большие данные, с другой стороны, характеризуются объемом, разнообразием и скоростью. Они требуют использования специализированных технологий и фреймворков для обработки и анализа. Большие данные могут быть как структурированными, так и неструктурированными, и включают:\n",
        "- Потоки данных с финансовых рынков в реальном времени\n",
        "- Социальные медиа и новостные ленты\n",
        "- Данные о поведении клиентов и транзакциях\n",
        "\n",
        "### Основные различия\n",
        "\n",
        "1. **Объем**: Малые данные обычно имеют ограниченный объем, в то время как большие данные могут достигать терабайтов и петабайтов.\n",
        "2. **Скорость**: Большие данные часто поступают в реальном времени, требуя мгновенной обработки, в отличие от малых данных, которые могут быть обработаны в пакетном режиме.\n",
        "3. **Разнообразие**: Большие данные могут включать текст, изображения, видео и другие форматы, тогда как малые данные обычно структурированы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoHy_QiWGe-c",
        "outputId": "b4fe81ad-2984-4e3e-991d-89a44c02dce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab environment detected.\n",
            "No display found. Using non-interactive Agg backend\n"
          ]
        }
      ],
      "source": [
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib, sys, os\n",
        "matplotlib.get_backend()\n",
        "#%matplotlib --list\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Colab environment detected.')\n",
        "if (os.environ.get('DISPLAY', '') == '') and (os.name != 'nt'):\n",
        "    print('No display found. Using non-interactive Agg backend')\n",
        "    mpl.use('Agg')\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "HoHy_QiWGe-c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX4ur4slhlYE",
        "outputId": "d9081c84-daa2-48b0-ef25-003cf717c34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype         \n",
            "---  ------    --------------  -----         \n",
            " 0   Date      5 non-null      datetime64[ns]\n",
            " 1   Revenue   5 non-null      int64         \n",
            " 2   Expenses  5 non-null      int64         \n",
            "dtypes: datetime64[ns](1), int64(2)\n",
            "memory usage: 248.0 bytes\n",
            "Малые данные:\n",
            "         Date  Revenue  Expenses\n",
            "0 2023-01-01      100        80\n",
            "1 2023-01-02      150       120\n",
            "2 2023-01-03      200       160\n",
            "3 2023-01-04      250       200\n",
            "4 2023-01-05      300       240 None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000000 entries, 0 to 99999999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Dtype         \n",
            "---  ------     -----         \n",
            " 0   Timestamp  datetime64[ns]\n",
            " 1   Value      object        \n",
            "dtypes: datetime64[ns](1), object(1)\n",
            "memory usage: 1.5+ GB\n",
            "\n",
            "Информация о наборе данных: None\n"
          ]
        }
      ],
      "source": [
        "small_data = pd.DataFrame({\n",
        "    'Date': pd.date_range(start='2023-01-01', periods=5, freq='D'),  # даты\n",
        "    'Revenue': [100, 150, 200, 250, 300],  # доходы\n",
        "    'Expenses': [80, 120, 160, 200, 240]  # расходы\n",
        "})\n",
        "\n",
        "print(\"Малые данные:\\n\",small_data,small_data.info())\n",
        "\n",
        "large_data = pd.DataFrame({\n",
        "    'Timestamp': pd.date_range(start='2023-01-01', periods=1e8, freq='T'),\n",
        "    'Value': [np.random.rand(1000000) * 1000.0]*int(1e18)\n",
        "})\n",
        "\n",
        "print(\"\\nИнформация о наборе данных:\", large_data.info())"
      ],
      "id": "gX4ur4slhlYE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "403Cpr54hlYg"
      },
      "source": [
        "# Управление данными и кэширование в финансовых приложениях\n",
        "\n",
        "В этом разделе мы рассмотрим, как использовать Redis и etcd для управления состоянием и кэшированием данных в финансовых приложениях. Эти [инструменты](https://db-engines.com/en/ranking) позволяют эффективно обрабатывать большие объемы данных, обеспечивая высокую производительность и надежность систем."
      ],
      "id": "403Cpr54hlYg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMu9Gxp9hlYh"
      },
      "source": [],
      "id": "JMu9Gxp9hlYh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlZZzJthlYi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get update\n",
        "!apt-get install -y redis-server\n",
        "!redis-server --daemonize yes # Запуск Redis в фоновом режиме\n",
        "!redis-server --version"
      ],
      "id": "_MlZZzJthlYi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMn_nyU_hlYj"
      },
      "source": [
        "## Пример использования Redis для кэширования данных\n",
        "\n",
        "В этом примере мы создадим простое приложение на Python, которое будет использовать Redis для кэширования результатов вычислений. Это позволит ускорить доступ к часто запрашиваемым данным.\n",
        "\n",
        "### Преимущества использования Redis в финансовых приложениях:\n",
        "- **Высокая скорость**: Данные хранятся в оперативной памяти, что обеспечивает быстрый доступ.\n",
        "- **Поддержка различных структур данных**: Позволяет хранить и обрабатывать данные в удобной форме.\n",
        "- **Масштабируемость**: Легко масштабируется для обработки больших объемов данных.\n",
        "- **Надежность**: Поддерживает механизмы репликации и персистентности данных."
      ],
      "id": "BMn_nyU_hlYj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_ijfp3H3sYJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install redis -q\n",
        "!pip install numpy==1.26.4 pandas==2.0.3 --force-reinstall\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import timeit\n",
        "import redis\n",
        "from math import sin\n",
        "\n",
        "r = redis.Redis(host='localhost', port=6379, db=0) # Создаем объект подключения к Redis"
      ],
      "id": "H_ijfp3H3sYJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT4EFHF4hlYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d82f0b-3685-4bd0-9109-e4b398e92930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-кеширование, 1-ый запуск\n",
            "1.48 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "+кеширование, 2-ый запуск\n",
            "224 µs ± 56.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "# Функция для вычисления и кэширования данных\n",
        "def expensive_computation(x):\n",
        "    # Проверяем, есть ли результат в кэше\n",
        "    if r.exists(f'computation:{x[0]}'):\n",
        "        #print('есть результат в кэше !')\n",
        "        return r.get(f'computation:{x[0]}')\n",
        "    else:\n",
        "        #print('нет в кеше, делаем вычисления')\n",
        "        result = float([sin(i**2) ** 2 for i in x][0])  # Пример сложных вычислений\n",
        "        r.set(f'computation:{x[0]}', result) # Сохраняем результат в кэше\n",
        "        return result\n",
        "\n",
        "print(f\"-кеширование, 1-ый запуск\")\n",
        "%timeit -n1 -r1 expensive_computation([5])\n",
        "print(f\"+кеширование, 2-ый запуск\")\n",
        "%timeit expensive_computation([5])"
      ],
      "id": "vT4EFHF4hlYk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioKAVqSFhlYl"
      },
      "source": [
        "## Пример использования etcd для кэширования данных\n",
        "\n",
        "etcd — это распределенное хранилище ключ-значение, которое обеспечивает надежное хранение данных и управление состоянием в распределенных системах. Оно часто используется для хранения конфигураций и координации сервисов.\n",
        "\n",
        "### Преимущества использования etcd в финансовых приложениях:\n",
        "- **Надежность**: Обеспечивает консистентность данных в распределенной среде.\n",
        "- **Высокая доступность**: Поддерживает кластеризацию и автоматическое восстановление.\n",
        "- **Простота использования**: Легко интегрируется с другими системами и сервисами.\n",
        "- **Безопасность**: Поддерживает аутентификацию и шифрование данных."
      ],
      "id": "ioKAVqSFhlYl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnAFhGH0hlYm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install -y etcd\n",
        "!pip install etcd3 protobuf==3.20.0"
      ],
      "id": "tnAFhGH0hlYm"
    },
    {
      "cell_type": "code",
      "source": [
        "from subprocess import Popen, PIPE\n",
        "proc = Popen(\"etcd --advertise-client-urls http://127.0.0.1:2379 --listen-client-urls http://127.0.0.1:2379\",\n",
        "    shell=True,\n",
        "    stdout=PIPE, stderr=PIPE\n",
        ")"
      ],
      "metadata": {
        "id": "dA5llqIxuoSD"
      },
      "id": "dA5llqIxuoSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOnBriS1KMhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb8fcf0-8d08-406e-e239-9567e7519e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "etcd Version: 3.3.25\n",
            "Git SHA: Not provided (use ./build instead of go build)\n",
            "Go Version: go1.18.1\n",
            "Go OS/Arch: linux/amd64\n"
          ]
        }
      ],
      "source": [
        "!etcd --version\n",
        "!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
      ],
      "id": "UOnBriS1KMhQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf2ATxIfShEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d4c234-a9b9-4c30-9144-484f867c6479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "member 8e9e05c52164694d is healthy: got healthy result from http://127.0.0.1:2379\n",
            "cluster is healthy\n"
          ]
        }
      ],
      "source": [
        "!etcdctl cluster-health #--help"
      ],
      "id": "sf2ATxIfShEE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1lDgmXnM0BL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e1e5f2-558d-412d-d0ce-6fd38b0c614d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ключ: это_ключ \n",
            "Значение: это_значение\n"
          ]
        }
      ],
      "source": [
        "import etcd3\n",
        "etcd = etcd3.client(\"127.0.0.1\")\n",
        "etcd.put('это_ключ', 'это_значение') #записали\n",
        "value, metadata = etcd.get('это_ключ') #считали\n",
        "print(\"Ключ: \" + metadata.key.decode('utf-8'), \"\\nЗначение: \" + value.decode('utf-8'))"
      ],
      "id": "K1lDgmXnM0BL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для вычисления и кэширования данных\n",
        "def expensive_computation(x):\n",
        "    # Проверяем, есть ли результат в кэше\n",
        "    try:\n",
        "        #print('есть результат в кэше !')\n",
        "        return etcd.get(f'computation:{x[0]}')\n",
        "    except:\n",
        "        #print('нет в кеше, делаем вычисления')\n",
        "        result = float([sin(i**2) ** 2 for i in x][0])  # Пример сложных вычислений\n",
        "        etcd.set(f'computation:{x[0]}', result) # Сохраняем результат в кэше\n",
        "        return result\n",
        "\n",
        "print(f\"-кеширование, 1-ый запуск\")\n",
        "%timeit -n1 -r1 expensive_computation([5])\n",
        "print(f\"+кеширование, 2-ый запуск\")\n",
        "%timeit expensive_computation([5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtBVU6E0y9LK",
        "outputId": "4fd21bf3-9c63-41d3-cb71-7cdaccfb222e"
      },
      "id": "UtBVU6E0y9LK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-кеширование, 1-ый запуск\n",
            "702 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "+кеширование, 2-ый запуск\n",
            "719 µs ± 209 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YeH9vkY81s"
      },
      "source": [
        "Рассмотрим процесс записи или набора ключа-значения в etcd:\n",
        "\n",
        "Шаг 1: Ведущий добавляет запись ключа-значения в журнал предварительной записи (WAL). Для этого действия требуется запись на диск с помощью функции fsync(), что влияет на задержку на диске.\n",
        "\n",
        "Шаг 2a: Ведущий уведомляет подписчиков об изменении. Это действие связано с сетевым взаимодействием, что влияет на задержку в сети.\n",
        "\n",
        "Шаг 2b: Подписчики добавляют запись в свой локальный WAL. Для этого действия требуется запись на диск с помощью функции fsync(), что приводит к задержке на диске.\n",
        "\n",
        "Шаг 2c: Подписчики уведомляют лидера о том, что они записали ключ-значение в свой WAL. Для этого действия требуется подключение к сети, что влияет на задержку в сети.\n",
        "\n",
        "Шаг 3: Лидер ожидает подтверждения от большинства (кворума) и фиксирует ввод ключа-значения. Для этого действия требуется еще одна запись на диск с помощью функции fsync(), что влияет на задержку на диске.\n",
        "\n",
        "Шаг 4a: Ведущий уведомляет подписчиков о том, что запись зафиксирована. Для этого действия требуется подключение к сети, что влияет на задержку в сети.\n",
        "\n",
        "Шаг 4b: После подтверждения от лидеров подписчики фиксируют ввод значения ключа. Для этого действия требуется запись на диск с помощью функции fsync(), что приводит к задержке на диске.\n",
        "\n",
        "<img src=\"https://www.redhat.com/rhdc/managed-files/ohc/WC%201.png\" width=\"50%\">"
      ],
      "id": "S-YeH9vkY81s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGVD_zIPhlYo"
      },
      "source": [
        "## Вывод\n",
        "\n",
        "Использование Redis и etcd в финансовых приложениях позволяет эффективно управлять данными и состоянием, обеспечивая высокую производительность и надежность систем. Эти инструменты помогают справляться с большими объемами данных и сложными задачами, характерными для финансовой индустрии."
      ],
      "id": "OGVD_zIPhlYo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQM5VyNKhlYp"
      },
      "source": [
        "# SQL базы данных с использованием графических процессоров (GPU)\n",
        "\n",
        "В этом разделе мы рассмотрим, как SQL базы данных могут использовать графические процессоры (GPU) для ускорения обработки данных. Мы обсудим преимущества использования GPU в финансовых приложениях и приведем примеры таких баз данных."
      ],
      "id": "IQM5VyNKhlYp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZwZUWAqhlYp"
      },
      "source": [
        "## Обзор SQL баз данных, использующих GPU\n",
        "\n",
        "Графические процессоры (GPU) изначально были разработаны для обработки графики и выполнения параллельных вычислений. Однако, благодаря их архитектуре, они также могут быть использованы для ускорения обработки данных в SQL базах данных. GPU могут выполнять тысячи потоков одновременно, что делает их идеальными для задач, требующих высокой степени параллелизма.\n",
        "\n",
        "### Примеры SQL баз данных с поддержкой GPU\n",
        "\n",
        "1. **BlazingSQL**: Это распределенная SQL база данных, которая использует GPU для ускорения обработки данных. BlazingSQL интегрируется с экосистемой RAPIDS, что позволяет выполнять анализ данных на GPU.\n",
        "\n",
        "2. **OmniSci (ранее MapD)**: Это аналитическая платформа, которая использует GPU для выполнения SQL запросов с высокой скоростью. OmniSci позволяет обрабатывать миллиарды строк данных в реальном времени.\n",
        "\n",
        "3. **Kinetica**: Это база данных, которая использует GPU для ускорения аналитических запросов. Kinetica поддерживает как SQL, так и NoSQL запросы, что делает ее универсальным решением для различных задач.\n",
        "\n",
        "### Преимущества использования GPU в SQL базах данных\n",
        "\n",
        "1. **Высокая производительность**: GPU могут обрабатывать данные значительно быстрее, чем традиционные CPU, благодаря своей способности выполнять множество операций параллельно.\n",
        "\n",
        "2. **Эффективность при работе с большими объемами данных**: GPU могут обрабатывать большие объемы данных за короткое время, что делает их идеальными для финансовых приложений, где скорость обработки данных критически важна.\n",
        "\n",
        "3. **Снижение затрат**: Использование GPU может снизить затраты на оборудование, так как одна GPU может заменить несколько CPU серверов.\n",
        "\n",
        "4. **Реализация сложных аналитических задач**: GPU позволяют выполнять сложные аналитические задачи, такие как машинное обучение и глубокое обучение, непосредственно в базе данных."
      ],
      "id": "cZwZUWAqhlYp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ktFe9CqhlYq"
      },
      "source": [
        "## Примеры использования GPU в финансовых приложениях\n",
        "\n",
        "Финансовая индустрия требует обработки огромных объемов данных в реальном времени. GPU могут значительно ускорить этот процесс, предоставляя аналитикам и трейдерам возможность принимать более обоснованные решения.\n",
        "\n",
        "### Пример 1: Анализ рыночных данных\n",
        "\n",
        "Финансовые компании могут использовать GPU для анализа рыночных данных в реальном времени. Это позволяет им быстро реагировать на изменения на рынке и принимать решения на основе актуальной информации.\n",
        "\n",
        "### Пример 2: Управление рисками\n",
        "\n",
        "GPU могут использоваться для моделирования и анализа рисков. Это позволяет финансовым учреждениям более точно оценивать риски и разрабатывать стратегии их минимизации.\n",
        "\n",
        "### Пример 3: Обнаружение мошенничества\n",
        "\n",
        "GPU могут ускорить процессы обнаружения мошенничества, анализируя транзакции в реальном времени и выявляя аномалии, которые могут указывать на мошенническую деятельность."
      ],
      "id": "4ktFe9CqhlYq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMX5rrz5hlYs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install cudf-cu11 cuml-cu11 cupy-cuda11x\n",
        "import pandas as pd"
      ],
      "id": "jMX5rrz5hlYs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6NvXvwu1_dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9caa9cc0-b2ae-42c8-d954-d27aee20f193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 244 entries, 0 to 243\n",
            "Data columns (total 7 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   total_bill  244 non-null    float64\n",
            " 1   tip         244 non-null    float64\n",
            " 2   sex         244 non-null    object \n",
            " 3   smoker      244 non-null    object \n",
            " 4   day         244 non-null    object \n",
            " 5   time        244 non-null    object \n",
            " 6   size        244 non-null    int64  \n",
            "dtypes: float64(2), int64(1), object(4)\n",
            "memory usage: 13.5+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(pd.read_csv(\"https://github.com/plotly/datasets/raw/master/tips.csv\").info())"
      ],
      "id": "h6NvXvwu1_dx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y38n4fT1xvQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9952caa6-e5b0-412d-ac3b-2c353380ee4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "запуск pandas\n",
            "476 µs ± 109 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "запуск cudf\n",
            "1.91 ms ± 259 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "pdtips_df = pd.read_csv(\"https://github.com/plotly/datasets/raw/master/tips.csv\")\n",
        "pdtips_df[\"tip_percentage\"] = pdtips_df[\"tip\"] / pdtips_df[\"total_bill\"] * 100\n",
        "\n",
        "print(f\"запуск pandas\")\n",
        "%timeit pdtips_df.groupby(\"size\").tip_percentage.mean()\n",
        "\n",
        "import cudf\n",
        "\n",
        "cutips_df = cudf.read_csv(\"https://github.com/plotly/datasets/raw/master/tips.csv\")\n",
        "cutips_df[\"tip_percentage\"] = cutips_df[\"tip\"] / cutips_df[\"total_bill\"] * 100\n",
        "\n",
        "print(f\"запуск cudf\")\n",
        "%timeit cutips_df.groupby(\"size\").tip_percentage.mean()\n"
      ],
      "id": "y38n4fT1xvQY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MukgYE_ohlYr"
      },
      "source": [
        "## Заключение\n",
        "\n",
        "Использование GPU в SQL базах данных предоставляет значительные преимущества для финансовой индустрии. Это позволяет обрабатывать большие объемы данных с высокой скоростью, снижать затраты и улучшать качество аналитики. Внедрение GPU в финансовые приложения может значительно повысить их эффективность и конкурентоспособность."
      ],
      "id": "MukgYE_ohlYr"
    },
    {
      "cell_type": "markdown",
      "id": "kCV9NyUjhlYN",
      "metadata": {
        "id": "kCV9NyUjhlYN"
      },
      "source": [
        "# Интеграция с Big Data фреймворками: Trino, Spark, Airflow\n",
        "\n",
        "В этом разделе мы рассмотрим три ключевых фреймворка для работы с большими данными: Trino, Apache Spark и Apache Airflow. Мы обсудим их особенности, преимущества и способы интеграции для обработки больших объемов данных в финансовой индустрии."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lzmfjy_hhlYR",
      "metadata": {
        "id": "lzmfjy_hhlYR"
      },
      "source": [
        "## Trino\n",
        "\n",
        "Trino (ранее известный как PrestoSQL) — это распределенный SQL-движок с открытым исходным кодом, который позволяет выполнять аналитические запросы к данным, хранящимся в различных источниках. Trino поддерживает множество коннекторов для работы с различными базами данных и хранилищами данных.\n",
        "\n",
        "### Основные преимущества Trino:\n",
        "- **Высокая производительность**: Trino оптимизирован для выполнения сложных аналитических запросов с минимальной задержкой.\n",
        "- **Масштабируемость**: Trino может обрабатывать данные, распределенные по множеству узлов, что позволяет масштабировать систему в зависимости от объема данных.\n",
        "- **Поддержка различных источников данных**: Trino поддерживает множество коннекторов для работы с различными базами данных и хранилищами данных, включая Hive, MySQL, PostgreSQL и другие."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "026Yz9wkhlYO",
      "metadata": {
        "id": "026Yz9wkhlYO"
      },
      "source": [
        "## Apache Spark\n",
        "\n",
        "Apache Spark — это распределенная вычислительная платформа с открытым исходным кодом, которая позволяет обрабатывать большие объемы данных с высокой скоростью. Spark поддерживает различные языки программирования, включая Python, Java и Scala, и предоставляет API для работы с данными в режиме реального времени и пакетной обработки.\n",
        "\n",
        "### Основные компоненты Apache Spark:\n",
        "- **Spark Core**: Основной компонент, отвечающий за распределенные вычисления и управление ресурсами.\n",
        "- **Spark SQL**: Модуль для работы с данными в формате SQL, который позволяет выполнять запросы к данным, хранящимся в различных источниках.\n",
        "- **Spark Streaming**: Модуль для обработки потоковых данных в реальном времени.\n",
        "- **MLlib**: Библиотека машинного обучения, предоставляющая алгоритмы для анализа данных.\n",
        "- **GraphX**: API для работы с графами и выполнения графовых вычислений."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1kbfprv8hlYT",
      "metadata": {
        "id": "1kbfprv8hlYT"
      },
      "source": [
        "## Apache Airflow\n",
        "\n",
        "Apache Airflow — это платформа для создания, планирования и мониторинга рабочих процессов. Airflow позволяет автоматизировать процессы обработки данных, создавая сложные пайплайны, которые могут выполняться по расписанию или в зависимости от событий.\n",
        "\n",
        "### Основные компоненты Apache Airflow:\n",
        "- **DAG (Directed Acyclic Graph)**: Основная структура, описывающая последовательность задач и их зависимости.\n",
        "- **Operators**: Компоненты, которые выполняют конкретные задачи, такие как запуск скриптов, перемещение данных и т.д.\n",
        "- **Scheduler**: Компонент, отвечающий за планирование и выполнение задач в соответствии с DAG.\n",
        "- **Web UI**: Веб-интерфейс для мониторинга и управления рабочими процессами."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MGfnJVnThlY4",
      "metadata": {
        "id": "MGfnJVnThlY4"
      },
      "source": [
        "# Практическая часть: Установка и настройка окружения\n",
        "\n",
        "В этом разделе мы установим и настроим окружение для работы с большими данными, используя популярные фреймворки и библиотеки. Мы будем работать в Google Colab, что позволяет нам использовать мощные вычислительные ресурсы без необходимости установки программного обеспечения на локальный компьютер."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P5EYPwiihlaa",
      "metadata": {
        "id": "P5EYPwiihlaa"
      },
      "source": [
        "## Установка Trino\n",
        "\n",
        "Trino — это распределенная база, который позволяет выполнять аналитические запросы на больших объемах данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GJ8rVY5rE2YI",
      "metadata": {
        "id": "GJ8rVY5rE2YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27792e3f-9bc6-41ca-bb29-5ac83d54d4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install trino -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNNKLLRLhlaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c787392-679c-4487-f646-36383b9bc019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"nodeVersion\":{\"version\":\"467\"},\"environment\":\"docker\",\"coordinator\":true,\"starting\":false,\"uptime\":\"21.04m\"}\n"
          ]
        }
      ],
      "source": [
        "!curl -s http://185.177.219.168:8085/v1/info # Проверка доступности сервера Trino"
      ],
      "id": "eNNKLLRLhlaj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6LHFL8z00IBG",
      "metadata": {
        "id": "6LHFL8z00IBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56127741-16f8-4f37-fd9e-4e7a8f9f920c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Посмотреть GUI http://185.177.219.168:8085/ui\n",
            "[['inside_trino', 'http://172.18.0.2:8080', '467', True, 'active']]\n"
          ]
        }
      ],
      "source": [
        "from trino.dbapi import connect\n",
        "import time\n",
        "\n",
        "host=\"185.177.219.168\"\n",
        "port=8085\n",
        "print(f\"Посмотреть GUI http://{host}:{port}/ui\")\n",
        "conn = connect(\n",
        "    host=host,\n",
        "    port=port,\n",
        "    user=\"user\",\n",
        "    catalog=\"1\",\n",
        "    schema=\"public\",\n",
        "    timezone='Europe/Moscow',\n",
        "    legacy_primitive_types=True,\n",
        ")\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT * FROM system.runtime.nodes\")\n",
        "rows = cur.fetchall()\n",
        "print(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSQdbShjJPsd",
      "metadata": {
        "id": "RSQdbShjJPsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4264247-17fa-4406-a308-31b48ccac851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['-2001-08-22']]\n"
          ]
        }
      ],
      "source": [
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT DATE '-2001-08-22'\")\n",
        "rows = cur.fetchall()\n",
        "print(rows)\n",
        "assert rows[0][0] == \"-2001-08-22\"\n",
        "assert cur.description[0][1] == \"date\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r6gTicmBqACp",
      "metadata": {
        "id": "r6gTicmBqACp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc1470b-da17-4e4f-d9e1-1cb3a3a23895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Async event loop already running. Adding coroutine to the event loop.\n",
            "0.0028867721557617188\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import os\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "async def hard_job():\n",
        "   while True:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT COUNT(*) FROM system.runtime.nodes\n",
        "            UNION ALL\n",
        "            \"\"\"+\" SELECT COUNT(*) FROM system.runtime.nodes\")\n",
        "        rows = cur.fetchone()\n",
        "\n",
        "        cur.execute(\"\"\"SELECT NOW()\"\"\")\n",
        "        rows = cur.fetchone()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  start = time.time()\n",
        "  try:\n",
        "      loop = asyncio.get_running_loop()\n",
        "  except RuntimeError:\n",
        "      loop = None\n",
        "  if loop and loop.is_running():\n",
        "      print('Async event loop already running. Adding coroutine to the event loop.')\n",
        "      tsk = loop.create_task(hard_job())\n",
        "      tsk.add_done_callback(lambda t: logging.info(f'Task done with result={t.result()}  << return val of main()'))\n",
        "  else:\n",
        "      logging.info('Starting new event loop')\n",
        "      result = asyncio.run(hard_job())\n",
        "  print(time.time()-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V78f_LZsG3f6",
      "metadata": {
        "id": "V78f_LZsG3f6"
      },
      "source": [
        "\n",
        "## Установка Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbNyzpphhlav",
      "metadata": {
        "id": "XbNyzpphhlav"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyspark -q # Устанавливаем PySpark, интерфейс для работы со Spark на Python\n",
        "!wget https://drive.google.com/uc?id=11V38AUYhTokBYaRf9Glr0isuqrSeuzqx -O /content/EURUSD_Candlestick_1_Hour_BID_01.07.2020-15.07.2023.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oEaf_Cvnhlaw",
      "metadata": {
        "id": "oEaf_Cvnhlaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff44d8d-ab57-488f-baae-f1e4eff5d2c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Gmt time: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession # Импортируем SparkSession из PySpark\n",
        "\n",
        "# Создаем SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"FinanceDataProcessing\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Указываем путь к нашему датасету\n",
        "s3_path = \"/content/EURUSD_Candlestick_1_Hour_BID_01.07.2020-15.07.2023.csv\"\n",
        "# Загружаем датасет в DataFrame\n",
        "sdf = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
        "\n",
        "# Выводим схему загруженного DataFrame\n",
        "sdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.select(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\").describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SqFwVdIDKp5",
        "outputId": "4722534e-539e-4d08-c595-208d51c28d6e"
      },
      "id": "_SqFwVdIDKp5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+-------------------+-------------------+--------------------+\n",
            "|summary|               Open|               High|                Low|              Close|              Volume|\n",
            "+-------+-------------------+-------------------+-------------------+-------------------+--------------------+\n",
            "|  count|              17768|              17768|              17768|              17768|               17768|\n",
            "|   mean| 1.1246986346240468| 1.1254486104232424| 1.1239660214993197| 1.1247025866726719|  11686.864068479525|\n",
            "| stddev|0.06896329376638866|0.06882445176700097|0.06908021172626208|0.06895785696887373|   16224.18962253226|\n",
            "|    min|             0.9539|            0.95592|            0.95357|             0.9539|2.293899999999999...|\n",
            "|    max|            1.23398|            1.23494|            1.23334|            1.23401|         688879.8125|\n",
            "+-------+-------------------+-------------------+-------------------+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# создадим гибрид pandas-on-Spark используя Spark DataFrame.\n",
        "psdf = sdf.pandas_api()\n",
        "psdf.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "BIURf_fCGZ-d",
        "outputId": "3a19b41b-4f46-4842-c4eb-8ce25a2d78a4"
      },
      "id": "BIURf_fCGZ-d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Open          High           Low         Close         Volume\n",
              "count  17768.000000  17768.000000  17768.000000  17768.000000   17768.000000\n",
              "mean       1.124699      1.125449      1.123966      1.124703   11686.864068\n",
              "std        0.068963      0.068824      0.069080      0.068958   16224.189623\n",
              "min        0.953900      0.955920      0.953570      0.953900       0.000229\n",
              "25%        1.071480      1.072190      1.070760      1.071460    2112.929900\n",
              "50%        1.132370      1.133000      1.131750      1.132420    6096.839800\n",
              "75%        1.183620      1.184310      1.183030      1.183620   14595.080100\n",
              "max        1.233980      1.234940      1.233340      1.234010  688879.812500"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17768.000000</td>\n",
              "      <td>17768.000000</td>\n",
              "      <td>17768.000000</td>\n",
              "      <td>17768.000000</td>\n",
              "      <td>17768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.124699</td>\n",
              "      <td>1.125449</td>\n",
              "      <td>1.123966</td>\n",
              "      <td>1.124703</td>\n",
              "      <td>11686.864068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.068963</td>\n",
              "      <td>0.068824</td>\n",
              "      <td>0.069080</td>\n",
              "      <td>0.068958</td>\n",
              "      <td>16224.189623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.953900</td>\n",
              "      <td>0.955920</td>\n",
              "      <td>0.953570</td>\n",
              "      <td>0.953900</td>\n",
              "      <td>0.000229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.071480</td>\n",
              "      <td>1.072190</td>\n",
              "      <td>1.070760</td>\n",
              "      <td>1.071460</td>\n",
              "      <td>2112.929900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.132370</td>\n",
              "      <td>1.133000</td>\n",
              "      <td>1.131750</td>\n",
              "      <td>1.132420</td>\n",
              "      <td>6096.839800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.183620</td>\n",
              "      <td>1.184310</td>\n",
              "      <td>1.183030</td>\n",
              "      <td>1.183620</td>\n",
              "      <td>14595.080100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.233980</td>\n",
              "      <td>1.234940</td>\n",
              "      <td>1.233340</td>\n",
              "      <td>1.234010</td>\n",
              "      <td>688879.812500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZsbmX6whlbL",
      "metadata": {
        "id": "uZsbmX6whlbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83dceee-9ab4-4a9c-820f-041a136ca15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "формат Arrow оптимизирует хранение данных в памяти, поддерживает быструю передачу данных по сети.\n",
            "выключить Arrow\n",
            "2.16 s ± 559 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "включить Arrow\n",
            "271 ms ± 30.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Exception in callback <lambda>(<Task finishe... by peer'))\")>) at <ipython-input-19-9d2666b1c2bf>:27\n",
            "handle: <Handle <lambda>(<Task finishe... by peer'))\")>) at <ipython-input-19-9d2666b1c2bf>:27>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 507, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 474, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py\", line 38, in reraise\n",
            "    raise value.with_traceback(tb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 507, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/client.py\", line 841, in fetch\n",
            "    response = self._request.get(self._request.next_uri)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/client.py\", line 573, in get\n",
            "    return self._get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/client.py\", line 911, in decorated\n",
            "    raise error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/client.py\", line 893, in decorated\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 602, in get\n",
            "    return self.request(\"GET\", url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 682, in send\n",
            "    raise ConnectionError(err, request=request)\n",
            "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"<ipython-input-19-9d2666b1c2bf>\", line 27, in <lambda>\n",
            "    tsk.add_done_callback(lambda t: logging.info(f'Task done with result={t.result()}  << return val of main()'))\n",
            "  File \"<ipython-input-19-9d2666b1c2bf>\", line 15, in hard_job\n",
            "    cur.execute(\"\"\"SELECT NOW()\"\"\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/dbapi.py\", line 585, in execute\n",
            "    self._iterator = iter(self._query.execute())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/client.py\", line 824, in execute\n",
            "    self._result.rows += self.fetch()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trino/client.py\", line 843, in fetch\n",
            "    raise trino.exceptions.TrinoConnectionError(\"failed to fetch: {}\".format(e))\n",
            "trino.exceptions.TrinoConnectionError: failed to fetch: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark.pandas as ps\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"PYARROW_IGNORE_TIMEZONE\"]=\"false\"\n",
        "pdf = pd.read_csv(s3_path)\n",
        "pdf.drop([\"Gmt time\",\"Volume\"],axis=1)\n",
        "\n",
        "prev = spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")\n",
        "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
        "print(\"формат Arrow оптимизирует хранение данных в памяти, поддерживает быструю передачу данных по сети.\")\n",
        "print(\"выключить Arrow\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", False)\n",
        "%timeit ps.range(300000).to_pandas()\n",
        "print(\"включить Arrow\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
        "%timeit ps.range(300000).to_pandas()\n",
        "ps.reset_option(\"compute.default_index_type\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", prev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a8IajmhlaZ",
      "metadata": {
        "id": "a4a8IajmhlaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "2f635036-6ab3-41dc-b6bd-e99ed75504df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b2674143ca0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3b9f1934501c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>FinanceDataProcessing</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"BigDataFinance\").getOrCreate() # Создаем локальную сессию\n",
        "spark # Выводим информацию о созданной сессии"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(psdf.head())\n",
        "def add(data):return data[2] / data[3]  #вычисляем индикатор High/Low\n",
        "# psdf[\"индикатор\"] = psdf.apply(add,axis=1) не сработает\n",
        "new_column = psdf.apply(add,axis=1)\n",
        "print(new_column.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwxsJemm_H7e",
        "outputId": "63922b7f-180d-4a0e-f954-3ad76226df14"
      },
      "id": "SwxsJemm_H7e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Gmt time     Open     High      Low    Close     Volume\n",
            "0  01.07.2020 00:00:00.000  1.12336  1.12336  1.12275  1.12306  4148.0298\n",
            "1  01.07.2020 01:00:00.000  1.12306  1.12395  1.12288  1.12385  5375.5801\n",
            "2  01.07.2020 02:00:00.000  1.12386  1.12406  1.12363  1.12382  4131.6099\n",
            "3  01.07.2020 03:00:00.000  1.12382  1.12388  1.12221  1.12265  4440.6001\n",
            "4  01.07.2020 04:00:00.000  1.12265  1.12272  1.12151  1.12179  4833.1001\n",
            "0    1.000543\n",
            "1    1.000953\n",
            "2    1.000383\n",
            "3    1.001488\n",
            "4    1.001079\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подитожим преимущества использования датафрейма PySpark по сравнению с Pandas:\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrame</font></font></th>\n",
        "<th><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrame</font></font></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrame поддерживает распараллеливание.&nbsp;</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrame не поддерживает распараллеливание.&nbsp;</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrame имеет несколько узлов.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrame имеет один узел.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\" class=\"\">Он следует принципу ленивого выполнения - это означает,<br> что задача не выполняется до тех пор, пока не будет выполнено действие.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Он следует принципу Eager Execution, что означает, что задача выполняется немедленно.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrame является неизменяемым.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrame является изменяемым.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Сложные операции сложнее выполнять по сравнению с Pandas DataFrame.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Сложные операции выполнять проще по сравнению со Spark DataFrame.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrame является распределенным, <br>поэтому обработка в Spark DataFrame больших объемов данных происходит быстрее.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrame не является распределенным, <br>поэтому обработка в Pandas DataFrame больших объемов данных будет выполняться медленнее.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">sparkDataFrame.count() возвращает количество строк.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">pandasDataFrame.count() возвращает количество наблюдений, отличных от NA/null, для каждого столбца.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrames отлично подходят для создания масштабируемых приложений.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrames нельзя использовать для создания масштабируемого приложения.</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Spark DataFrame обеспечивает отказоустойчивость.</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Pandas DataFrame не гарантирует отказоустойчивость. <br>Для ее обеспечения нам необходимо реализовать собственную структуру.</font></font></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "В этом разделе мы рассмотрели, как загружать и предварительно обрабатывать большие финансовые датасеты с использованием PySpark, а также как использовать Trino для выполнения распределенных SQL-запросов. Эти инструменты позволяют эффективно работать с большими объемами данных и извлекать из них полезную информацию."
      ],
      "metadata": {
        "id": "OgW5aF-INC46"
      },
      "id": "OgW5aF-INC46"
    },
    {
      "cell_type": "markdown",
      "id": "Lmp85_3Lhlac",
      "metadata": {
        "id": "Lmp85_3Lhlac"
      },
      "source": [
        "## Установка Apache Airflow\n",
        "\n",
        "Apache Airflow — это платформа для создания, планирования и мониторинга рабочих процессов. Мы установим Airflow с помощью pip и настроим его для работы в нашем окружении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zymP57Nrhla4",
      "metadata": {
        "id": "zymP57Nrhla4"
      },
      "outputs": [],
      "source": [
        "# Загрузка и запуск контейнера с сервером Apache Airflow\n",
        "#mkdir -p ./dags ./logs ./plugins\n",
        "#curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.9.0/docker-compose.yaml'\n",
        "#echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n",
        "#docker compose up\n",
        "# Airflow будет доступен по адресу http://localhost:8080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24Hq8I3hlad",
      "metadata": {
        "id": "e24Hq8I3hlad"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Установка Apache Airflow\n",
        "!pip install 'apache-airflow==2.10.1' \\\n",
        " --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.1/constraints-3.8.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oH3Vz4-Xhlak",
      "metadata": {
        "id": "oH3Vz4-Xhlak"
      },
      "outputs": [],
      "source": [
        "!curl -s http://185.177.219.168:8080/health  # Используем curl для проверки доступности Airflow и выводим информацию о его состоянии"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NCPV5-lkhla5",
      "metadata": {
        "id": "NCPV5-lkhla5"
      },
      "source": [
        "### Создание простого пайплайна в Airflow\n",
        "\n",
        "Теперь, когда Airflow запущен, мы можем создать простой пайплайн. Пайплайн будет состоять из трех задач: загрузка данных, обработка данных и сохранение результатов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DcSztyA9hla7",
      "metadata": {
        "id": "DcSztyA9hla7"
      },
      "source": [
        "### Мониторинг и управление процессами в Airflow\n",
        "\n",
        "Airflow предоставляет удобный веб-интерфейс для мониторинга и управления процессами. Вы можете просматривать состояние задач, логи выполнения и управлять расписанием выполнения DAG."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4SOXhxYqhla7",
      "metadata": {
        "id": "4SOXhxYqhla7"
      },
      "source": [
        "### Доступ к веб-интерфейсу Airflow\n",
        "\n",
        "Откройте браузер и перейдите по адресу [http://localhost:8080](http://localhost:8080). Вы увидите веб-интерфейс Airflow, где сможете управлять созданным пайплайном."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WnAg08rGhla8",
      "metadata": {
        "id": "WnAg08rGhla8"
      },
      "source": [
        "### Управление задачами\n",
        "\n",
        "В веб-интерфейсе вы можете:\n",
        "- Запускать и останавливать DAG\n",
        "- Просматривать состояние задач\n",
        "- Просматривать логи выполнения задач\n",
        "- Изменять расписание выполнения DAG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CPCZfZHVhla-",
      "metadata": {
        "id": "CPCZfZHVhla-"
      },
      "source": [
        "### Заключение\n",
        "\n",
        "В этом разделе мы рассмотрели, как использовать Apache Airflow для автоматизации процессов обработки данных. Мы создали простой пайплайн, состоящий из трех задач, и научились управлять процессами через веб-интерфейс Airflow. Это лишь базовое введение в возможности Airflow, и вы можете расширять и усложнять пайплайны в зависимости от ваших потребностей."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установка Apache Ignite"
      ],
      "metadata": {
        "id": "TXOkyb4zOc6s"
      },
      "id": "TXOkyb4zOc6s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Одиночный сервер**"
      ],
      "metadata": {
        "id": "njdqdUuQOmc0"
      },
      "id": "njdqdUuQOmc0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "sudo docker pull apacheignite/ignite\n",
        "sudo docker run -d apacheignite/ignite\n",
        "#sudo docker run -p 10800:10800 -p 8080:8080 apacheignite/ignite\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "oAnGE7wQFpCh"
      },
      "id": "oAnGE7wQFpCh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Кластер**\n",
        "\n",
        "docker-compose.yml\n",
        "\n",
        "```\n",
        "services:\n",
        "  ignite_0:\n",
        "    image: apacheignite/ignite:latest\n",
        "    ports:\n",
        "      - 10800:10800\n",
        "    restart: always\n",
        "\n",
        "  ignite_1:\n",
        "    image: apacheignite/ignite:latest\n",
        "    ports:\n",
        "      - 10801:10800\n",
        "    restart: always\n",
        "\n",
        "  ignite_2:\n",
        "    image: apacheignite/ignite:latest\n",
        "    ports:\n",
        "      - 10802:10800\n",
        "    restart: always\n",
        "```"
      ],
      "metadata": {
        "id": "TiP9546bLwe0"
      },
      "id": "TiP9546bLwe0"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyignite -q"
      ],
      "metadata": {
        "id": "r14h9EVhF38s"
      },
      "execution_count": null,
      "outputs": [],
      "id": "r14h9EVhF38s"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyignite import Client\n",
        "\n",
        "client = Client( use_ssl=False)\n",
        "with client.connect('185.177.219.168', 10800):\n",
        "    #my_cache = client.create_cache(cache_config) # так сломается, потому что уже один раз создали\n",
        "    my_cache = client.get_or_create_cache('new_cache')\n",
        "    my_cache.put('my key', 42)\n",
        "\n",
        "    result = my_cache.get('my key')\n",
        "    print(result)  # 42\n",
        "\n",
        "    result = my_cache.get('non-existent key')\n",
        "    print(result)  # None\n",
        "\n",
        "    result = my_cache.get_all([\n",
        "        'my key',\n",
        "        'non-existent key',\n",
        "        'other-key',\n",
        "    ])\n",
        "    print(result)  # {'my key': 42}\n",
        "\n",
        "    my_cache.clear_key('my key')\n",
        "\n",
        "    my_cache.destroy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E3DjsKoHXwH",
        "outputId": "d8dde7ee-e4be-47b7-b332-a23d9bc6e893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n",
            "None\n",
            "{'my key': 42}\n"
          ]
        }
      ],
      "id": "0E3DjsKoHXwH"
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from pyignite import Client\n",
        "\n",
        "client = Client()\n",
        "with client.connect('185.177.219.168', 10800):\n",
        "    my_cache = client.create_cache('my cache')\n",
        "    my_cache.put_all({'key_{}'.format(v): v for v in range(20)})\n",
        "\n",
        "\n",
        "    with my_cache.scan() as cursor:\n",
        "        for k, v in cursor:\n",
        "            print(k, v)\n",
        "\n",
        "    with my_cache.scan() as cursor:\n",
        "        pprint(dict(cursor))\n",
        "\n",
        "    my_cache.destroy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGuJHr-9LRqL",
        "outputId": "2f06f7ee-f431-45a4-b712-f926e3342786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key_3 3\n",
            "key_2 2\n",
            "key_1 1\n",
            "key_0 0\n",
            "key_7 7\n",
            "key_6 6\n",
            "key_5 5\n",
            "key_4 4\n",
            "key_9 9\n",
            "key_8 8\n",
            "key_19 19\n",
            "key_17 17\n",
            "key_18 18\n",
            "key_15 15\n",
            "key_16 16\n",
            "key_13 13\n",
            "key_14 14\n",
            "key_11 11\n",
            "key_12 12\n",
            "key_10 10\n",
            "{'key_0': 0,\n",
            " 'key_1': 1,\n",
            " 'key_10': 10,\n",
            " 'key_11': 11,\n",
            " 'key_12': 12,\n",
            " 'key_13': 13,\n",
            " 'key_14': 14,\n",
            " 'key_15': 15,\n",
            " 'key_16': 16,\n",
            " 'key_17': 17,\n",
            " 'key_18': 18,\n",
            " 'key_19': 19,\n",
            " 'key_2': 2,\n",
            " 'key_3': 3,\n",
            " 'key_4': 4,\n",
            " 'key_5': 5,\n",
            " 'key_6': 6,\n",
            " 'key_7': 7,\n",
            " 'key_8': 8,\n",
            " 'key_9': 9}\n"
          ]
        }
      ],
      "id": "cGuJHr-9LRqL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://ignite.apache.org/img/usecases/high-peformance/perf-pic.svg\">"
      ],
      "metadata": {
        "id": "VLsTjV_Ng-nC"
      },
      "id": "VLsTjV_Ng-nC"
    },
    {
      "cell_type": "code",
      "source": [
        "from decimal import Decimal\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class TableNames(Enum):\n",
        "    COUNTRY_TABLE_NAME = 'Country'\n",
        "    CITY_TABLE_NAME = 'City'\n",
        "    LANGUAGE_TABLE_NAME = 'CountryLanguage'\n",
        "\n",
        "\n",
        "class Query:\n",
        "    COUNTRY_CREATE_TABLE = '''CREATE TABLE Country (\n",
        "        Code CHAR(3) PRIMARY KEY,\n",
        "        Name CHAR(52),\n",
        "        Continent CHAR(50),\n",
        "        Region CHAR(26),\n",
        "        SurfaceArea DECIMAL(10,2),\n",
        "        IndepYear SMALLINT(6),\n",
        "        Population INT(11),\n",
        "        LifeExpectancy DECIMAL(3,1),\n",
        "        GNP DECIMAL(10,2),\n",
        "        GNPOld DECIMAL(10,2),\n",
        "        LocalName CHAR(45),\n",
        "        GovernmentForm CHAR(45),\n",
        "        HeadOfState CHAR(60),\n",
        "        Capital INT(11),\n",
        "        Code2 CHAR(2)\n",
        "    )'''\n",
        "\n",
        "    COUNTRY_INSERT = '''INSERT INTO Country(\n",
        "        Code, Name, Continent, Region,\n",
        "        SurfaceArea, IndepYear, Population,\n",
        "        LifeExpectancy, GNP, GNPOld,\n",
        "        LocalName, GovernmentForm, HeadOfState,\n",
        "        Capital, Code2\n",
        "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'''\n",
        "\n",
        "    CITY_CREATE_TABLE = '''CREATE TABLE City (\n",
        "        ID INT(11),\n",
        "        Name CHAR(35),\n",
        "        CountryCode CHAR(3),\n",
        "        District CHAR(20),\n",
        "        Population INT(11),\n",
        "        PRIMARY KEY (ID, CountryCode)\n",
        "    ) WITH \"affinityKey=CountryCode\"'''\n",
        "\n",
        "    CITY_CREATE_INDEX = 'CREATE INDEX idx_country_code ON city (CountryCode)'\n",
        "\n",
        "    CITY_INSERT = '''INSERT INTO City(\n",
        "        ID, Name, CountryCode, District, Population\n",
        "    ) VALUES (?, ?, ?, ?, ?)'''\n",
        "\n",
        "    LANGUAGE_CREATE_TABLE = '''CREATE TABLE CountryLanguage (\n",
        "        CountryCode CHAR(3),\n",
        "        Language CHAR(30),\n",
        "        IsOfficial BOOLEAN,\n",
        "        Percentage DECIMAL(4,1),\n",
        "        PRIMARY KEY (CountryCode, Language)\n",
        "    ) WITH \"affinityKey=CountryCode\"'''\n",
        "\n",
        "    LANGUAGE_CREATE_INDEX = 'CREATE INDEX idx_lang_country_code ON CountryLanguage (CountryCode)'\n",
        "\n",
        "    LANGUAGE_INSERT = '''INSERT INTO CountryLanguage(\n",
        "        CountryCode, Language, IsOfficial, Percentage\n",
        "    ) VALUES (?, ?, ?, ?)'''\n",
        "\n",
        "    DROP_TABLE = 'DROP TABLE {} IF EXISTS'\n",
        "\n",
        "\n",
        "class TestData:\n",
        "    COUNTRY = [\n",
        "        [\n",
        "            'USA', 'United States', 'North America', 'North America',\n",
        "            Decimal('9363520.00'), 1776, 278357000,\n",
        "            Decimal('77.1'), Decimal('8510700.00'), Decimal('8110900.00'),\n",
        "            'United States', 'Federal Republic', 'George W. Bush',\n",
        "            3813, 'US',\n",
        "        ],\n",
        "        [\n",
        "            'IND', 'India', 'Asia', 'Southern and Central Asia',\n",
        "            Decimal('3287263.00'), 1947, 1013662000,\n",
        "            Decimal('62.5'), Decimal('447114.00'), Decimal('430572.00'),\n",
        "            'Bharat/India', 'Federal Republic', 'Kocheril Raman Narayanan',\n",
        "            1109, 'IN',\n",
        "        ],\n",
        "        [\n",
        "            'CHN', 'China', 'Asia', 'Eastern Asia',\n",
        "            Decimal('9572900.00'), -1523, 1277558000,\n",
        "            Decimal('71.4'), Decimal('982268.00'), Decimal('917719.00'),\n",
        "            'Zhongquo', 'PeoplesRepublic', 'Jiang Zemin',\n",
        "            1891, 'CN',\n",
        "        ],\n",
        "    ]\n",
        "\n",
        "    CITY = [\n",
        "        [3793, 'New York', 'USA', 'New York', 8008278],\n",
        "        [3794, 'Los Angeles', 'USA', 'California', 3694820],\n",
        "        [3795, 'Chicago', 'USA', 'Illinois', 2896016],\n",
        "        [3796, 'Houston', 'USA', 'Texas', 1953631],\n",
        "        [3797, 'Philadelphia', 'USA', 'Pennsylvania', 1517550],\n",
        "        [3798, 'Phoenix', 'USA', 'Arizona', 1321045],\n",
        "        [3799, 'San Diego', 'USA', 'California', 1223400],\n",
        "        [3800, 'Dallas', 'USA', 'Texas', 1188580],\n",
        "        [3801, 'San Antonio', 'USA', 'Texas', 1144646],\n",
        "        [3802, 'Detroit', 'USA', 'Michigan', 951270],\n",
        "        [3803, 'San Jose', 'USA', 'California', 894943],\n",
        "        [3804, 'Indianapolis', 'USA', 'Indiana', 791926],\n",
        "        [3805, 'San Francisco', 'USA', 'California', 776733],\n",
        "        [1024, 'Mumbai (Bombay)', 'IND', 'Maharashtra', 10500000],\n",
        "        [1025, 'Delhi', 'IND', 'Delhi', 7206704],\n",
        "        [1026, 'Calcutta [Kolkata]', 'IND', 'West Bengali', 4399819],\n",
        "        [1027, 'Chennai (Madras)', 'IND', 'Tamil Nadu', 3841396],\n",
        "        [1028, 'Hyderabad', 'IND', 'Andhra Pradesh', 2964638],\n",
        "        [1029, 'Ahmedabad', 'IND', 'Gujarat', 2876710],\n",
        "        [1030, 'Bangalore', 'IND', 'Karnataka', 2660088],\n",
        "        [1031, 'Kanpur', 'IND', 'Uttar Pradesh', 1874409],\n",
        "        [1032, 'Nagpur', 'IND', 'Maharashtra', 1624752],\n",
        "        [1033, 'Lucknow', 'IND', 'Uttar Pradesh', 1619115],\n",
        "        [1034, 'Pune', 'IND', 'Maharashtra', 1566651],\n",
        "        [1035, 'Surat', 'IND', 'Gujarat', 1498817],\n",
        "        [1036, 'Jaipur', 'IND', 'Rajasthan', 1458483],\n",
        "        [1890, 'Shanghai', 'CHN', 'Shanghai', 9696300],\n",
        "        [1891, 'Peking', 'CHN', 'Peking', 7472000],\n",
        "        [1892, 'Chongqing', 'CHN', 'Chongqing', 6351600],\n",
        "        [1893, 'Tianjin', 'CHN', 'Tianjin', 5286800],\n",
        "        [1894, 'Wuhan', 'CHN', 'Hubei', 4344600],\n",
        "        [1895, 'Harbin', 'CHN', 'Heilongjiang', 4289800],\n",
        "        [1896, 'Shenyang', 'CHN', 'Liaoning', 4265200],\n",
        "        [1897, 'Kanton [Guangzhou]', 'CHN', 'Guangdong', 4256300],\n",
        "        [1898, 'Chengdu', 'CHN', 'Sichuan', 3361500],\n",
        "        [1899, 'Nanking [Nanjing]', 'CHN', 'Jiangsu', 2870300],\n",
        "        [1900, 'Changchun', 'CHN', 'Jilin', 2812000],\n",
        "        [1901, 'Xi´an', 'CHN', 'Shaanxi', 2761400],\n",
        "        [1902, 'Dalian', 'CHN', 'Liaoning', 2697000],\n",
        "        [1903, 'Qingdao', 'CHN', 'Shandong', 2596000],\n",
        "        [1904, 'Jinan', 'CHN', 'Shandong', 2278100],\n",
        "        [1905, 'Hangzhou', 'CHN', 'Zhejiang', 2190500],\n",
        "        [1906, 'Zhengzhou', 'CHN', 'Henan', 2107200],\n",
        "    ]\n",
        "\n",
        "    LANGUAGE = [\n",
        "        ['USA', 'Chinese', False, Decimal('0.6')],\n",
        "        ['USA', 'English', True, Decimal('86.2')],\n",
        "        ['USA', 'French', False, Decimal('0.7')],\n",
        "        ['USA', 'German', False, Decimal('0.7')],\n",
        "        ['USA', 'Italian', False, Decimal('0.6')],\n",
        "        ['USA', 'Japanese', False, Decimal('0.2')],\n",
        "        ['USA', 'Korean', False, Decimal('0.3')],\n",
        "        ['USA', 'Polish', False, Decimal('0.3')],\n",
        "        ['USA', 'Portuguese', False, Decimal('0.2')],\n",
        "        ['USA', 'Spanish', False, Decimal('7.5')],\n",
        "        ['USA', 'Tagalog', False, Decimal('0.4')],\n",
        "        ['USA', 'Vietnamese', False, Decimal('0.2')],\n",
        "        ['IND', 'Asami', False, Decimal('1.5')],\n",
        "        ['IND', 'Bengali', False, Decimal('8.2')],\n",
        "        ['IND', 'Gujarati', False, Decimal('4.8')],\n",
        "        ['IND', 'Hindi', True, Decimal('39.9')],\n",
        "        ['IND', 'Kannada', False, Decimal('3.9')],\n",
        "        ['IND', 'Malajalam', False, Decimal('3.6')],\n",
        "        ['IND', 'Marathi', False, Decimal('7.4')],\n",
        "        ['IND', 'Orija', False, Decimal('3.3')],\n",
        "        ['IND', 'Punjabi', False, Decimal('2.8')],\n",
        "        ['IND', 'Tamil', False, Decimal('6.3')],\n",
        "        ['IND', 'Telugu', False, Decimal('7.8')],\n",
        "        ['IND', 'Urdu', False, Decimal('5.1')],\n",
        "        ['CHN', 'Chinese', True, Decimal('92.0')],\n",
        "        ['CHN', 'Dong', False, Decimal('0.2')],\n",
        "        ['CHN', 'Hui', False, Decimal('0.8')],\n",
        "        ['CHN', 'Mantšu', False, Decimal('0.9')],\n",
        "        ['CHN', 'Miao', False, Decimal('0.7')],\n",
        "        ['CHN', 'Mongolian', False, Decimal('0.4')],\n",
        "        ['CHN', 'Puyi', False, Decimal('0.2')],\n",
        "        ['CHN', 'Tibetan', False, Decimal('0.4')],\n",
        "        ['CHN', 'Tujia', False, Decimal('0.5')],\n",
        "        ['CHN', 'Uighur', False, Decimal('0.6')],\n",
        "        ['CHN', 'Yi', False, Decimal('0.6')],\n",
        "        ['CHN', 'Zhuang', False, Decimal('1.4')],\n",
        "    ]\n",
        "\n",
        "from pyignite import Client\n",
        "\n",
        "# establish connection\n",
        "client = Client()\n",
        "with client.connect('185.177.219.168', 10800):\n",
        "    # create tables\n",
        "    for query in [\n",
        "        Query.COUNTRY_CREATE_TABLE,\n",
        "        Query.CITY_CREATE_TABLE,\n",
        "        Query.LANGUAGE_CREATE_TABLE,\n",
        "    ]:\n",
        "        client.sql(query)\n",
        "\n",
        "    # create indices\n",
        "    for query in [Query.CITY_CREATE_INDEX, Query.LANGUAGE_CREATE_INDEX]:\n",
        "        client.sql(query)\n",
        "\n",
        "    # load data\n",
        "    for row in TestData.COUNTRY:\n",
        "        client.sql(Query.COUNTRY_INSERT, query_args=row)\n",
        "\n",
        "    for row in TestData.CITY:\n",
        "        client.sql(Query.CITY_INSERT, query_args=row)\n",
        "\n",
        "    for row in TestData.LANGUAGE:\n",
        "        client.sql(Query.LANGUAGE_INSERT, query_args=row)\n",
        "\n",
        "    # 10 most populated cities (with pagination)\n",
        "    with client.sql('SELECT name, population FROM City ORDER BY population DESC LIMIT 10') as cursor:\n",
        "        print('Most 10 populated cities:')\n",
        "        for row in cursor:\n",
        "            print(row)\n",
        "    # Most 10 populated cities:\n",
        "    # ['Mumbai (Bombay)', 10500000]\n",
        "    # ['Shanghai', 9696300]\n",
        "    # ['New York', 8008278]\n",
        "    # ['Peking', 7472000]\n",
        "    # ['Delhi', 7206704]\n",
        "    # ['Chongqing', 6351600]\n",
        "    # ['Tianjin', 5286800]\n",
        "    # ['Calcutta [Kolkata]', 4399819]\n",
        "    # ['Wuhan', 4344600]\n",
        "    # ['Harbin', 4289800]\n",
        "    print('-' * 20)\n",
        "    # 10 most populated cities in 3 countries (with pagination and header row)\n",
        "    MOST_POPULATED_IN_3_COUNTRIES = '''\n",
        "    SELECT country.name as country_name, city.name as city_name, MAX(city.population) AS max_pop FROM country\n",
        "        JOIN city ON city.countrycode = country.code\n",
        "        WHERE country.code IN ('USA','IND','CHN')\n",
        "        GROUP BY country.name, city.name ORDER BY max_pop DESC LIMIT 10\n",
        "    '''\n",
        "\n",
        "    with client.sql(MOST_POPULATED_IN_3_COUNTRIES, include_field_names=True) as cursor:\n",
        "        print('Most 10 populated cities in USA, India and China:')\n",
        "        table_str_pattern = '{:15}\\t| {:20}\\t| {}'\n",
        "        print(table_str_pattern.format(*next(cursor)))\n",
        "        print('*' * 50)\n",
        "        for row in cursor:\n",
        "            print(table_str_pattern.format(*row))\n",
        "    # Most 10 populated cities in USA, India and China:\n",
        "    # COUNTRY_NAME   \t| CITY_NAME           \t| MAX_POP\n",
        "    # **************************************************\n",
        "    # India          \t| Mumbai (Bombay)     \t| 10500000\n",
        "    # China          \t| Shanghai            \t| 9696300\n",
        "    # United States  \t| New York            \t| 8008278\n",
        "    # China          \t| Peking              \t| 7472000\n",
        "    # India          \t| Delhi               \t| 7206704\n",
        "    # China          \t| Chongqing           \t| 6351600\n",
        "    # China          \t| Tianjin             \t| 5286800\n",
        "    # India          \t| Calcutta [Kolkata]  \t| 4399819\n",
        "    # China          \t| Wuhan               \t| 4344600\n",
        "    # China          \t| Harbin              \t| 4289800\n",
        "    print('-' * 20)\n",
        "\n",
        "    # Show city info\n",
        "    with client.sql('SELECT * FROM City WHERE id = ?', query_args=[3802], include_field_names=True) as cursor:\n",
        "        field_names = next(cursor)\n",
        "        field = list(*cursor)\n",
        "        print('City info:')\n",
        "        for field_name, field_value in zip(field_names * len(field), field):\n",
        "            print(f'{field_name}: {field_value}')\n",
        "    # City info:\n",
        "    # ID: 3802\n",
        "    # NAME: Detroit\n",
        "    # COUNTRYCODE: USA\n",
        "    # DISTRICT: Michigan\n",
        "    # POPULATION: 951270\n",
        "\n",
        "    # Clean up\n",
        "    for table_name in TableNames:\n",
        "        result = client.sql(Query.DROP_TABLE.format(table_name.value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79cde54-0969-4dbc-9eb3-85478d03b8e1",
        "id": "bt9HoztkAMta"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most 10 populated cities:\n",
            "['Mumbai (Bombay)', 10500000]\n",
            "['Shanghai', 9696300]\n",
            "['New York', 8008278]\n",
            "['Peking', 7472000]\n",
            "['Delhi', 7206704]\n",
            "['Chongqing', 6351600]\n",
            "['Tianjin', 5286800]\n",
            "['Calcutta [Kolkata]', 4399819]\n",
            "['Wuhan', 4344600]\n",
            "['Harbin', 4289800]\n",
            "--------------------\n",
            "Most 10 populated cities in USA, India and China:\n",
            "COUNTRY_NAME   \t| CITY_NAME           \t| MAX_POP\n",
            "**************************************************\n",
            "India          \t| Mumbai (Bombay)     \t| 10500000\n",
            "China          \t| Shanghai            \t| 9696300\n",
            "United States  \t| New York            \t| 8008278\n",
            "China          \t| Peking              \t| 7472000\n",
            "India          \t| Delhi               \t| 7206704\n",
            "China          \t| Chongqing           \t| 6351600\n",
            "China          \t| Tianjin             \t| 5286800\n",
            "India          \t| Calcutta [Kolkata]  \t| 4399819\n",
            "China          \t| Wuhan               \t| 4344600\n",
            "China          \t| Harbin              \t| 4289800\n",
            "--------------------\n",
            "City info:\n",
            "ID: 3802\n",
            "NAME: Detroit\n",
            "COUNTRYCODE: USA\n",
            "DISTRICT: Michigan\n",
            "POPULATION: 951270\n"
          ]
        }
      ],
      "id": "bt9HoztkAMta"
    },
    {
      "cell_type": "code",
      "source": [
        "import pyignite\n",
        "from pyignite.exceptions import SQLError\n",
        "\n",
        "from time import time\n",
        "from uuid import uuid4\n",
        "from datetime import datetime\n",
        "from random import randint, uniform\n",
        "\n",
        "session = pyignite.Client(handshake_timeout=20.0)\n",
        "session.connect('185.177.219.168', 10800)\n",
        "\n",
        "create_table=\"\"\"CREATE TABLE IF NOT EXISTS public.test(\n",
        "\ttrans_id varchar(255) not NULL PRIMARY key,\n",
        "\tamount int not null,\n",
        "\tdecimal_amount decimal(19, 3) not null,\n",
        "\tclient varchar(255) not null,\n",
        "\toperation_type varchar(10) not null,\n",
        "\tex_text_field varchar not null,\n",
        "\ttransaction_ts timestamp not null,\n",
        "\ttransaction_dt date not null\n",
        ");\"\"\"\n",
        "\n",
        "session.sql(create_table)\n",
        "LIMIT = 1_000#_000\n",
        "def gen_test_data():\n",
        "    _test_data = []\n",
        "    for index in range(1, LIMIT+1):\n",
        "        data = {\n",
        "            \"trans_id\": f\"C{uuid4().hex.upper()}\",\n",
        "            \"amount\": randint(100, 200),\n",
        "            \"decimal_amount\": uniform(0.00001, 25.123),\n",
        "            \"client\": f\"FFM{randint(115, 720)}\",\n",
        "            \"operation_type\": \"C2C\",\n",
        "            \"ex_text_field\": \"pyignite.exceptions\",\n",
        "            \"transaction_ts\": datetime.now(),\n",
        "            \"transaction_dt\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "        }\n",
        "        _test_data.append(list(data.values()))\n",
        "    return _test_data\n",
        "\n",
        "\n",
        "\n",
        "def insert_(data: tuple, table_name: str):\n",
        "    columns = \",\".join([\n",
        "        \"trans_id\", \"amount\", \"decimal_amount\", \"client\", \"operation_type\", \"ex_text_field\", \"transaction_ts\", \"transaction_dt\"\n",
        "        ])\n",
        "    cmd = f\"insert into {table_name}({columns}) values (?, ?, ?, ?, ?, ?, ?, ?)\"\n",
        "    try:\n",
        "        session.sql(cmd, query_args=data)\n",
        "    except SQLError:\n",
        "        pass\n",
        "\n",
        "def speedtest(data: list):\n",
        "    session.sql(\"delete from public.test;\")\n",
        "\n",
        "    start = time()\n",
        "    for row in data:\n",
        "        #print(row)\n",
        "        insert_(row, \"public.test\")\n",
        "\n",
        "    finish = (time() - start)+1e-10\n",
        "    print(f\"Прошло времени, ss: {round(finish, 3)} // Кол-во {len(data)} // RPS: {round(len(data) / round(finish))}\")\n",
        "\n",
        "\n",
        "speedtest(gen_test_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A35m48qW5K_s",
        "outputId": "18ce1ea3-4808-4e7b-bc04-8a9e831e1b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Прошло времени, ss: 222.1 // Кол-во 1000 // RPS: 5\n"
          ]
        }
      ],
      "id": "A35m48qW5K_s"
    },
    {
      "cell_type": "markdown",
      "id": "-CA_BCwshlbL",
      "metadata": {
        "id": "-CA_BCwshlbL"
      },
      "source": [
        "# Заключение\n",
        "\n",
        "Инструменты больших данных позволяют эффективно управлять большими объемами данных и обеспечивать высокую производительность обработки."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}